#!/usr/bin/env bash
# This script is generated by the entrypoint.sh script
# Do not edit it manually
set -e

# Go to the scraper directory
cd "$SCRAPERDIR"

# Create a temporary log file so we can analyze the output
if [[ -n "$GITHUB_OUTPUT" ]]; then
  tmplog="$(mktemp -t indexing.log.XXXXXX)"
  trap 'rm -f $tmplog' EXIT
else
  tmplog="/dev/null"
fi

# Run the indexing
pipenv run python -m src.index 2>&1 | tee "$tmplog"

if [[ -n "$GITHUB_OUTPUT" ]]; then
  # If we are running in GitHub context, let's set some
  # output variables that can then be used in the workflow

  # Get the total number of records
  total_records="$(cat "$tmplog" | \
    grep -oP '(?<=^Nb hits: )[0-9]*$' || echo 0)"
  echo "total_records=${total_records}" | tee -a "$GITHUB_OUTPUT"

  # Get the pages without records
  without_record=($(cat "$tmplog" | \
    sed 's/\x1b\[[0-9;]*m//g; s/[^[:print:]\n]//g' | \
    grep -oP 'https?://.*(?= 0 records)' || true))

  echo "pages_without_record_count=${#without_record[@]}" | tee -a "$GITHUB_OUTPUT"
  echo "pages_without_record<<EOF" | tee -a "$GITHUB_OUTPUT"
  for page in "${without_record[@]}"; do
    echo "$page" | tee -a "$GITHUB_OUTPUT"
  done
  echo "EOF" | tee -a "$GITHUB_OUTPUT"

  # Get the pages with errors
  with_errors=($(cat "$tmplog" | \
		grep -oP '(?<=Spider error processing <GET ).*(?=>)' || true))

  echo "pages_with_errors_count=${#with_errors[@]}" | tee -a "$GITHUB_OUTPUT"
  echo "pages_with_errors<<EOF" | tee -a "$GITHUB_OUTPUT"
  for page in "${with_errors[@]}"; do
    echo "$page" | tee -a "$GITHUB_OUTPUT"
  done
  echo "EOF" | tee -a "$GITHUB_OUTPUT"

  # Get the number of records that are too big
  too_big_records=($(cat "$tmplog" | \
    grep -oP '(?<=objectID=)[^ ]*(?= is too big)' || true))
  echo "too_big_records_count=${#too_big_records[@]}" | tee -a "$GITHUB_OUTPUT"

  # Cleanup
  rm -f \$tmplog
fi
